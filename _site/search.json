[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Can Prediction Error Explain Predictability Effects on the N1 during Picture-Word Verification?",
    "section": "",
    "text": "Jack E. Taylor1,2, Sara C. Sereno2, Guillaume, A. Rousselet2 \n\n1Department of Psychology, Goethe University Frankfurt\n2School of Psychology and Neuroscience, University of Glasgow"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "abstract.html",
    "href": "abstract.html",
    "title": "Abstract",
    "section": "",
    "text": "Predictive coding models posit that brain activity scales with prediction error: the difference between ascending (bottom-up) input and descending (top-down) predictions. Prediction error captures two key variables: magnitude and certainty of the ascending-descending difference. Greater violations of expectations should elicit greater prediction error signals, and this effect should become larger as certainty increases. We asked whether such a simple predictive coding account could describe neural activity indexed by the N1 (~170 ms) event-related potential component elicited by words. Indeed, findings have shown that the word-related N1 is sensitive to predictions, with unpredicted words generally eliciting greater-amplitude N1s than predicted words. However, effects of error magnitude and certainty have mostly been investigated in isolation, providing incomplete tests of predictive coding. In our pre-registered study, we tested the account via the interaction between prediction congruency (error magnitude) and predictability (certainty). We recorded electroencephalograms for 68 participants while they completed a picture-word verification paradigm. PICTURE-word pairs were congruent (e.g., ONION-onion) or incongruent (e.g., ONION-torch), while predictability was manipulated continuously based on norms of picture-name association (% name agreement). Pre-registered analyses failed to find evidence that the direction of the congruency-predictability interaction matched that expected under a simple predictive coding account. Exploratory Bayesian analyses found strong evidence against the account, with the congruency-predictability interaction 51.52* times more likely in the opposite direction. Specifically, higher predictability elicited larger N1s for picture-congruent words, and smaller N1s for picture-incongruent words. We argue that a simple predictive coding account of the N1 is either incorrect or requires elaboration.\n\n\n\n\n\n\n* “51.52 times more likely”\n\n\n\nAfter we refit the model using a random seed for reproducibility, this estimate changed to 59.98"
  },
  {
    "objectID": "stimuli.html",
    "href": "stimuli.html",
    "title": "Stimuli",
    "section": "",
    "text": "Picture-Word stimuli comprised pictures from the BOSS norms, and matched pairs of congruent and incongruent words, with predictability varied continuously.\nA total of 400 words were selected with the R package, LexOPS (Taylor et al., 2020). There were 200 words per Congruency condition, with one congruent and one incongruent word per image. Images were taken from the Bank of Online Standardised Stimuli (BOSS) Norms (Brodeur et al., 2014). Congruent words were images’ modal names; Incongruent words were matched item-wise to each Congruent word.\n\nWords were selected…\n\nto have high word prevalence (Brysbaert et al., 2019) such that ≥90% of people knew each word.\nto be nouns, according to dominant part of speech in SUBTLEX-UK (van Heuven et al., 2014).\nto have a mean concreteness rating >4 (on a 1-5 scale; Brysbaert et al., 2014)\n\n\nPicture-Congruent and -Incongruent words were matched on:\n\nWord length (number of characters), exactly.\nConcreteness according to Brysbaert et al. (2014), within ±.25\nZipf frequency (a logarithmic scale of word frequency) according to SUBTLEX-UK (van Heuven et al., 2014), within ±.125\nCharacter bigram probability (calculated from SUBTLEX-UK), within ±.0025\nOLD20 (the average Orthographic Levenshtein Distance of the 20 closest neighbours to a given word; Yarkoni et al., 2008) calculated from the LexOPS inbuilt dataset, within ±.75.\n\n\nMatched pairs of Picture-Congruent and -Incongruent words also had:\n\nMaximal distance from one another in orthographic Levenshtein distance.\nA cosine PPMI semantic similarity value of ≤.01 according to the Small World of Words (De Deyne et al., 2019)\n\n\nEach participant saw Set 1 or Set 2 of the stimuli, which determined which images preceded congruent, and which incongruent, words. These counterbalanced sets of stimuli were matched via overlap (Pastore & Calcagnì, 2019), to maximise distributional similarity in:\n\nPercentage of modal name agreement according to the BOSS norms\nCosine PPMI semantic similarity according to the SWOW\nZipf word frequency according to SUBTLEX-UK\nCharacter bigram probability according to SUBTLEX-UK\nWord concreteness (Brysbaert et al., 2014)\nWord length\nOLD20 (Orthographic neighbourhood density)\n\n\n\n\nPicture-Word Stimuli Summary"
  },
  {
    "objectID": "stimuli.html#localiser-stimuli",
    "href": "stimuli.html#localiser-stimuli",
    "title": "Stimuli",
    "section": "Localiser Stimuli",
    "text": "Localiser Stimuli\n\n\n\n\n\nLocaliser stimuli comprised matched triplets of words (Courier New font), false-font strings (BACS2serif font; Vidal et al., 2017), and phase-shuffled words. The pre-registered comparison was between words and false-font strings; phase-shuffled words were included as an exploratory comparison.\nDistributional similarity of the words to various variables was maximised via distributional overlap (Pastore & Calcagnì, 2019), as shown below.\n\n\n\n\n\nPhase-shuffled words were generated by randomly permuting the phase distribution of images of words. A GitHub repository implementing this method is available here: https://github.com/JackEdTaylor/randphase"
  },
  {
    "objectID": "localiser.html",
    "href": "localiser.html",
    "title": "Localiser Results",
    "section": "",
    "text": "We also modelled results from the localiser task. We modelled both right- and left-hemispheric occipitotemporal electrodes via per-sample linear mixed-effects models. Results (below) showed clear differences in the N1 between the stimuli, in both amplitude and latency.\n\n\n\n(A) Model Estimates. (B) Model ERP predictions."
  }
]